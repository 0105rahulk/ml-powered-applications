{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel.ameisen/ml_editor/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from data_processing import format_raw_df\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/writers.csv')\n",
    "df = format_raw_df(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import get_random_train_test_split, get_vectorized_inputs_and_label\n",
    "\n",
    "train_df_rand, test_df_rand = get_random_train_test_split(df[df[\"is_question\"]], test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/ml_editor/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vectors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-58f24e932ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO update train_df_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectorized_inputs_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_rand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectorized_inputs_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df_rand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/oreilly/code/ml-editor/data_processing.py\u001b[0m in \u001b[0;36mget_vectorized_inputs_and_label\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \"\"\"\n\u001b[1;32m     98\u001b[0m     vectorized_features = np.append(\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vectors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         df[\n\u001b[1;32m    101\u001b[0m             [\n",
      "\u001b[0;32m~/ml_editor/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml_editor/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vectors'"
     ]
    }
   ],
   "source": [
    "# TODO update train_df_rand\n",
    "\n",
    "X_train, y_train = get_vectorized_inputs_and_label(train_df_rand)\n",
    "\n",
    "X_test, y_test = get_vectorized_inputs_and_label(test_df_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = clf.predict(X_test)\n",
    "y_predicted_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "# Thanks to https://datascience.stackexchange.com/questions/13151/randomforestclassifier-oob-scoring-method\n",
    "y_train_pred = np.argmax(clf.oob_decision_function_,axis=1)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_train, y_train_pred)\n",
    "print(\"Training accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted)\n",
    "print(\"Validation accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import get_confusion_matrix_plot\n",
    "\n",
    "get_confusion_matrix_plot(y_predicted, y_test, figsize=(9, 9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import get_roc_plot\n",
    "\n",
    "get_roc_plot(y_predicted_proba[:,1], y_test, figsize=(10,10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_roc_plot(y_predicted_proba[:,1], y_test, fpr_bar=.1, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import get_calibration_plot\n",
    "\n",
    "get_calibration_plot(y_predicted_proba[:,1], y_test, figsize=(9,9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import get_feature_importance\n",
    "\n",
    "feature_names = [\n",
    "    \"action_verb_full\",\n",
    "    \"question_mark_full\",\n",
    "    \"norm_text_len\",\n",
    "    \"language_question\",\n",
    "]\n",
    "\n",
    "w_indices = [\"word_vector_index_%s\" % s for s in range(300)]\n",
    "w_indices.extend(feature_names)\n",
    "all_feature_names = np.array(w_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Top 5 importances:\\n\")\n",
    "print('\\n'.join([\"%s: %.2g\" % (tup[0], tup[1]) for tup in get_feature_importance(clf, all_feature_names)[:5]]))\n",
    "\n",
    "print(\"\\nBottom 5 importances:\\n\")\n",
    "print('\\n'.join([\"%s: %.2g\" % (tup[0], tup[1]) for tup in get_feature_importance(clf, all_feature_names)[-5:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at most and least successful examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import get_top_k\n",
    "test_analysis_df = test_df_rand.copy()\n",
    "test_analysis_df[\"predicted_proba\"] = y_predicted_proba[:, 1]\n",
    "test_analysis_df[\"true_label\"] = y_test\n",
    "\n",
    "to_display = [\n",
    "    \"predicted_proba\",\n",
    "    \"true_label\",\n",
    "    \"Title\",\n",
    "    \"body_text\",\n",
    "    \"text_len\",\n",
    "    \"action_verb_full\",\n",
    "    \"question_mark_full\",\n",
    "    \"language_question\",\n",
    "]\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "top_pos, top_neg, worst_pos, worst_neg, unsure = get_top_k(test_analysis_df, \"predicted_proba\", \"true_label\", k=2)\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most confident correct positive predictions\n",
    "top_pos[to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most confident correct negative predictions\n",
    "top_neg[to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most confident incorrect negative predictions\n",
    "worst_pos[to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most confident incorrect positive predictions\n",
    "worst_neg[to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most unsure questions\n",
    "unsure[to_display]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "vector_store = nlp\n",
    "\n",
    "clf_text_only = RandomForestClassifier(n_estimators=100, class_weight='balanced', oob_score=True)\n",
    "X_train_text = np.vstack(train_df_rand[\"full_text\"].apply(lambda x: nlp(x).vector))\n",
    "X_test_text = np.vstack(test_df_rand[\"full_text\"].apply(lambda x: nlp(x).vector))\n",
    "clf_text_only.fit(X_train_text, y_train)\n",
    "\n",
    "def text_pipeline(examples):\n",
    "    global vector_store\n",
    "    vectors = [nlp(x).vector for x in examples]\n",
    "    vectors=np.vstack(np.array(vectors))\n",
    "\n",
    "    return clf_text_only.predict_proba(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_one_instance(instance, class_names):\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(instance, text_pipeline, num_features=6)\n",
    "    return exp\n",
    "\n",
    "def visualize_one_exp(features, labels, index, class_names = [\"Unanswered\",\"Answered\"]):\n",
    "    exp = explain_one_instance(features[index], class_names = class_names)\n",
    "    print('Index: %d' % index)\n",
    "    print('True class: %s' % class_names[labels[index]])\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(list(test_df_rand[\"full_text\"]), list(y_test), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(40)\n",
    "\n",
    "def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n",
    "    sample_sentences = random.sample(test_set, sample_size)\n",
    "    explainer = LimeTextExplainer()\n",
    "    \n",
    "    labels_to_sentences = defaultdict(list)\n",
    "    contributors = defaultdict(dict)\n",
    "    \n",
    "    # First, find contributing words to each class\n",
    "    for sentence in sample_sentences:\n",
    "        probabilities = word2vec_pipeline([sentence])\n",
    "        curr_label = probabilities[0].argmax()\n",
    "        labels_to_sentences[curr_label].append(sentence)\n",
    "        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n",
    "        listed_explanation = exp.as_list(label=curr_label)\n",
    "        \n",
    "        for word,contributing_weight in listed_explanation:\n",
    "            if word in contributors[curr_label]:\n",
    "                contributors[curr_label][word].append(contributing_weight)\n",
    "            else:\n",
    "                contributors[curr_label][word] = [contributing_weight]    \n",
    "    \n",
    "    # average each word's contribution to a class, and sort them by impact\n",
    "    average_contributions = {}\n",
    "    sorted_contributions = {}\n",
    "    for label,lexica in contributors.items():\n",
    "        curr_label = label\n",
    "        curr_lexica = lexica\n",
    "        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n",
    "        for word,scores in curr_lexica.items():\n",
    "            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n",
    "        detractors = average_contributions[curr_label].sort_values()\n",
    "        supporters = average_contributions[curr_label].sort_values(ascending=False)\n",
    "        sorted_contributions[label_dict[curr_label]] = {\n",
    "            'detractors':detractors,\n",
    "             'supporters': supporters\n",
    "        }\n",
    "    return sorted_contributions\n",
    "\n",
    "label_to_text = {\n",
    "    0: 'Unanswered',\n",
    "    1: 'Answered',\n",
    "}\n",
    "sorted_contributions = get_statistical_explanation(list(test_df_rand[\"full_text\"]), 5, text_pipeline, label_to_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Unanswered', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Answered', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = sorted_contributions['Answered']['supporters'][:10].index.tolist()\n",
    "top_scores = sorted_contributions['Answered']['supporters'][:10].tolist()\n",
    "bottom_words = sorted_contributions['Answered']['detractors'][:10].index.tolist()\n",
    "bottom_scores = sorted_contributions['Answered']['detractors'][:10].tolist()\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try a simple model\n",
    "\n",
    "# TODO update train_df_rand\n",
    "\n",
    "X_train, y_train = get_vectorized_inputs_and_label(train_df_rand)\n",
    "\n",
    "X_test, y_test = get_vectorized_inputs_and_label(test_df_rand)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = clf.predict(X_test)\n",
    "y_predicted_proba = clf.predict_proba(X_test)\n",
    "\n",
    "y_train.value_counts()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "# Training accuracy\n",
    "# Thanks to https://datascience.stackexchange.com/questions/13151/randomforestclassifier-oob-scoring-method\n",
    "y_train_pred = np.argmax(clf.oob_decision_function_,axis=1)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_train, y_train_pred)\n",
    "print(\"Training accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted)\n",
    "print(\"Validation accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "\n",
    "from model_evaluation import get_confusion_matrix_plot\n",
    "\n",
    "get_confusion_matrix_plot(y_predicted, y_test, figsize=(9, 9))\n",
    "plt.show()\n",
    "\n",
    "from model_evaluation import get_roc_plot\n",
    "\n",
    "get_roc_plot(y_predicted_proba[:,1], y_test, figsize=(10,10))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "get_roc_plot(y_predicted_proba[:,1], y_test, fpr_bar=.1, figsize=(10,10))\n",
    "plt.show()\n",
    "\n",
    "from model_evaluation import get_calibration_plot\n",
    "\n",
    "get_calibration_plot(y_predicted_proba[:,1], y_test, figsize=(9,9))\n",
    "\n",
    "\n",
    "from model_evaluation import get_feature_importance\n",
    "\n",
    "feature_names = [\n",
    "    \"action_verb_full\",\n",
    "    \"question_mark_full\",\n",
    "    \"norm_text_len\",\n",
    "    \"language_question\",\n",
    "]\n",
    "\n",
    "w_indices = [\"word_vector_index_%s\" % s for s in range(300)]\n",
    "w_indices.extend(feature_names)\n",
    "all_feature_names = np.array(w_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top 5 importances:\\n\")\n",
    "print('\\n'.join([\"%s: %.2g\" % (tup[0], tup[1]) for tup in get_feature_importance(clf, all_feature_names)[:5]]))\n",
    "\n",
    "print(\"\\nBottom 5 importances:\\n\")\n",
    "print('\\n'.join([\"%s: %.2g\" % (tup[0], tup[1]) for tup in get_feature_importance(clf, all_feature_names)[-5:]]))\n",
    "\n",
    "## Let's look at most and least successful examples\n",
    "\n",
    "from model_evaluation import get_top_k\n",
    "test_analysis_df = test_df_rand.copy()\n",
    "test_analysis_df[\"predicted_proba\"] = y_predicted_proba[:, 1]\n",
    "test_analysis_df[\"true_label\"] = y_test\n",
    "\n",
    "to_display = [\n",
    "    \"predicted_proba\",\n",
    "    \"true_label\",\n",
    "    \"Title\",\n",
    "    \"body_text\",\n",
    "    \"text_len\",\n",
    "    \"action_verb_full\",\n",
    "    \"question_mark_full\",\n",
    "    \"language_question\",\n",
    "]\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "top_pos, top_neg, worst_pos, worst_neg, unsure = get_top_k(test_analysis_df, \"predicted_proba\", \"true_label\", k=2)\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# Most confident correct positive predictions\n",
    "top_pos[to_display]\n",
    "\n",
    "# Most confident correct negative predictions\n",
    "top_neg[to_display]\n",
    "\n",
    "# Most confident incorrect negative predictions\n",
    "worst_pos[to_display]\n",
    "\n",
    "# Most confident incorrect positive predictions\n",
    "worst_neg[to_display]\n",
    "\n",
    "# Most unsure questions\n",
    "unsure[to_display]\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "vector_store = nlp\n",
    "\n",
    "clf_text_only = RandomForestClassifier(n_estimators=100, class_weight='balanced', oob_score=True)\n",
    "X_train_text = np.vstack(train_df_rand[\"full_text\"].apply(lambda x: nlp(x).vector))\n",
    "X_test_text = np.vstack(test_df_rand[\"full_text\"].apply(lambda x: nlp(x).vector))\n",
    "clf_text_only.fit(X_train_text, y_train)\n",
    "\n",
    "def text_pipeline(examples):\n",
    "    global vector_store\n",
    "    vectors = [nlp(x).vector for x in examples]\n",
    "    vectors=np.vstack(np.array(vectors))\n",
    "\n",
    "    return clf_text_only.predict_proba(vectors)\n",
    "\n",
    "\n",
    "def explain_one_instance(instance, class_names):\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(instance, text_pipeline, num_features=6)\n",
    "    return exp\n",
    "\n",
    "def visualize_one_exp(features, labels, index, class_names = [\"Unanswered\",\"Answered\"]):\n",
    "    exp = explain_one_instance(features[index], class_names = class_names)\n",
    "    print('Index: %d' % index)\n",
    "    print('True class: %s' % class_names[labels[index]])\n",
    "    exp.show_in_notebook(text=True)\n",
    "\n",
    "visualize_one_exp(list(test_df_rand[\"full_text\"]), list(y_test), 7)\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(40)\n",
    "\n",
    "def get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n",
    "    sample_sentences = random.sample(test_set, sample_size)\n",
    "    explainer = LimeTextExplainer()\n",
    "    \n",
    "    labels_to_sentences = defaultdict(list)\n",
    "    contributors = defaultdict(dict)\n",
    "    \n",
    "    # First, find contributing words to each class\n",
    "    for sentence in sample_sentences:\n",
    "        probabilities = word2vec_pipeline([sentence])\n",
    "        curr_label = probabilities[0].argmax()\n",
    "        labels_to_sentences[curr_label].append(sentence)\n",
    "        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n",
    "        listed_explanation = exp.as_list(label=curr_label)\n",
    "        \n",
    "        for word,contributing_weight in listed_explanation:\n",
    "            if word in contributors[curr_label]:\n",
    "                contributors[curr_label][word].append(contributing_weight)\n",
    "            else:\n",
    "                contributors[curr_label][word] = [contributing_weight]    \n",
    "    \n",
    "    # average each word's contribution to a class, and sort them by impact\n",
    "    average_contributions = {}\n",
    "    sorted_contributions = {}\n",
    "    for label,lexica in contributors.items():\n",
    "        curr_label = label\n",
    "        curr_lexica = lexica\n",
    "        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n",
    "        for word,scores in curr_lexica.items():\n",
    "            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n",
    "        detractors = average_contributions[curr_label].sort_values()\n",
    "        supporters = average_contributions[curr_label].sort_values(ascending=False)\n",
    "        sorted_contributions[label_dict[curr_label]] = {\n",
    "            'detractors':detractors,\n",
    "             'supporters': supporters\n",
    "        }\n",
    "    return sorted_contributions\n",
    "\n",
    "label_to_text = {\n",
    "    0: 'Unanswered',\n",
    "    1: 'Answered',\n",
    "}\n",
    "sorted_contributions = get_statistical_explanation(list(test_df_rand[\"full_text\"]), 5, text_pipeline, label_to_text)\n",
    "\n",
    "\n",
    "\n",
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Unanswered', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Answered', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "top_words = sorted_contributions['Answered']['supporters'][:10].index.tolist()\n",
    "top_scores = sorted_contributions['Answered']['supporters'][:10].tolist()\n",
    "bottom_words = sorted_contributions['Answered']['detractors'][:10].index.tolist()\n",
    "bottom_scores = sorted_contributions['Answered']['detractors'][:10].tolist()\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_editor",
   "language": "python",
   "name": "ml_editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
