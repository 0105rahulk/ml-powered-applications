{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vectorize our data to explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to split data\n",
    "\n",
    "#### Random Split\n",
    "\n",
    "First, let's split our data randomly in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel.ameisen/ml_editor/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/emmanuel.ameisen/ml_editor/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_processing import format_raw_df\n",
    "\n",
    "df = pd.read_csv('data/writers.csv')\n",
    "df = format_raw_df(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import get_random_train_test_split, get_vectorized_inputs_and_label\n",
    "\n",
    "train_df_rand, test_df_rand = get_random_train_test_split(df[df[\"is_question\"]], test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6376, 1595)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_rand), len(test_df_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author Split\n",
    "\n",
    "Some authors may be more successful on average, and that may due to factors other than the quality of their formulation such as their popularity. To remove this potential source of bias, we could split by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5676 questions in training, 2295 in test.\n",
      "2723 different owners in the training set\n",
      "1167 different owners in the testing set\n",
      "0 overlapping owners\n"
     ]
    }
   ],
   "source": [
    "from data_processing import get_split_by_author\n",
    "\n",
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])\n",
    "\n",
    "print(\"%s questions in training, %s in test.\" % (len(train_author),len(test_author)))\n",
    "train_owners = set(train_author['OwnerUserId'].values)\n",
    "test_owners = set(test_author['OwnerUserId'].values)\n",
    "print(\"%s different owners in the training set\" % len(train_owners))\n",
    "print(\"%s different owners in the testing set\" % len(test_owners))\n",
    "print(\"%s overlapping owners\" % len(train_owners.intersection(test_owners)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's vectorize our text\n",
    "\n",
    "#### TF-IDF on ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2907\n",
       "False    2769\n",
       "Name: AcceptedAnswerId, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "questions = train_author[train_author[\"is_question\"]]\n",
    "raw_text = questions[\"body_text\"]\n",
    "sent_labels = questions[\"AcceptedAnswerId\"].notna()\n",
    "\n",
    "sent_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5676, 27381)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of a tfidf vectorizer, \n",
    "# We could use CountVectorizer for a non normalized version\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=2**21)\n",
    "\n",
    "# Fit our vectorizer to questions in our dataset\n",
    "# Returns an array of vectorized text\n",
    "bag_of_words = vectorizer.fit_transform(raw_text)\n",
    "\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_embedder = umap.UMAP()\n",
    "umap_bow = umap_embedder.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings):\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    color_map = {\n",
    "        True: '#1f77b4',\n",
    "        False:'#ff7f0e'\n",
    "    }\n",
    "    plt.scatter(embeddings[:, 0], embeddings[:, 1], \n",
    "                c=[color_map[x] for x in sent_labels], \n",
    "              s=40, alpha=0.4)\n",
    "\n",
    "    handles = [Rectangle((0, 0), 1, 1, color=c, ec=\"k\") for c in\n",
    "               ['#1f77b4', '#ff7f0e']]\n",
    "    labels = [\"answered\", \"unanswered\"]\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    plt.gca().set_xlabel(\"x\")\n",
    "    plt.gca().set_ylabel(\"y\")\n",
    "\n",
    "plot_embeddings(umap_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import umap\n",
    "import numpy as np \n",
    "# Load a large model, and disable pipeline unnecessary parts for our task\n",
    "# This speeds up the vectorization process significantly\n",
    "# See https://spacy.io/models/en#en_core_web_lg for details about the model\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for each of our questions\n",
    "# By default, the vector returned is the average of all vectors in the sentence\n",
    "# See https://spacy.io/usage/vectors-similarity for more\n",
    "spacy_emb = train_author[train_author[\"is_question\"]][\"body_text\"].apply(lambda x: nlp(x).vector)\n",
    "embeddings = np.vstack(spacy_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = list(spacy_emb)\n",
    "\n",
    "umap_embedder = umap.UMAP()\n",
    "umap_emb = umap_embedder.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, our different classes do not seem that well separated at all, let's go through the data and see if we can find any patterns to create a few features that could be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10, Category10\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def get_interactive_umap_embeddings_plot(umap_vectors, labels, text, legends, tooltip_label=None):\n",
    "    if not tooltip_label:\n",
    "        print(\"Using standard label\")\n",
    "        tooltip_label = labels\n",
    "    w2v_df = pd.DataFrame(umap_vectors, columns=('x', 'y'))\n",
    "    print(len(w2v_df))\n",
    "    w2v_df['label'] = [str(x) for x in labels]\n",
    "    w2v_df['tooltip_label'] = [str(x) for x in tooltip_label]\n",
    "    w2v_df['text'] = list(text)\n",
    "    w2v_df['legends'] = [\"Answered\" if el else \"Unanswered\" for el in list(legends)]\n",
    "    datasource = ColumnDataSource(w2v_df)\n",
    "\n",
    "    color_mapping = CategoricalColorMapper(factors=['True','False'], palette=['#1f77b4', '#ff7f0e'])\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"text\", \"@text\"),\n",
    "        ('got_answer', '@tooltip_label')\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=TOOLTIPS)\n",
    "    hover.attachment ='right'\n",
    "\n",
    "    plot_figure = figure(\n",
    "        title='UMAP projection of questions',\n",
    "        plot_width=900,\n",
    "        plot_height=600,\n",
    "        tools=('pan, wheel_zoom, reset', 'box_zoom', 'undo')\n",
    "    )\n",
    "    plot_figure.add_tools(hover)\n",
    "    \n",
    "    plot_figure.circle(\n",
    "        'x',\n",
    "        'y',\n",
    "        source=datasource,\n",
    "        color=dict(field='label', transform=color_mapping),\n",
    "        legend='legends',\n",
    "        line_alpha=0,\n",
    "        fill_alpha=0.4,\n",
    "        size=5\n",
    "    )\n",
    "    return plot_figure\n",
    "\n",
    "# plot_figure = get_interactive_umap_embeddings_plot(umap_emb, sent_labels, raw_text, legends=sent_labels)\n",
    "# show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"body_text_question\"].fillna(\"\", inplace=True)\n",
    "\n",
    "def show_question_features_containing(text):\n",
    "    return df[df[\"body_text_question\"].str.contains(text)][[\"body_text\", \"CommentCount\", \"text_len\", \n",
    "                                                             \"body_text_question\", \"text_len_question\", \n",
    "                                                       \"Score_question\", \"AcceptedAnswerId_question\"]]\n",
    "\n",
    "# Good example of two similar questions\n",
    "show_question_features_containing(\"I'm an amateur writer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential features\n",
    "\n",
    "Looking at the examples above, we can already think of a few features that could make our model better. To start with, we could generate features for:\n",
    "- question length\n",
    "- presence of question mark\n",
    "- vocabulary associated with a clear question (action verbs, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action_verb\"] = (df[\"body_text\"].str.contains(\"can\", regex=False) | df[\"body_text\"].str.contains(\"What\", regex=False) | df[\"body_text\"].str.contains(\"should\", regex=False)).astype(int)\n",
    "df[\"question_mark\"] = df[\"body_text\"].str.contains(\"?\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action_verb\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question_mark\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"norm_text_len\"]= get_normalized_series(df, \"text_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending our generated features to word features\n",
    "\n",
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])\n",
    "vectorized_features = np.append(np.array(embeddings), train_author[train_author[\"is_question\"]][[\"action_verb\",\"question_mark\", \n",
    "                                                                            \"norm_text_len\"]], 1)\n",
    "\n",
    "vectorized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_features = umap_embedder.fit_transform(vectorized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_figure = get_interactive_umap_embeddings_plot(umap_features, sent_labels, raw_text, legends=sent_labels)\n",
    "# show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_question_features_containing(\"capitalize\")\n",
    "show_question_features_containing(\"abbreviate\")\n",
    "show_question_features_containing(\"Specifically, how to describe\")\n",
    "\n",
    "# The title actually describes a lot of information\n",
    "df[df[\"body_text\"].str.contains(\"Specifically, how to describe\", regex=False)][[\"body_text\", \"Title\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language_question\"] = (df[\"body_text\"].str.contains(\"punctuate\", regex=False) | df[\"body_text\"].str.contains(\"capitalize\", regex=False) | df[\"body_text\"].str.contains(\"abbreviate\", regex=False)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_text\"] = df[\"Title\"].str.cat(df[\"body_text\"], sep=' ', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action_verb_full\"] = (df[\"full_text\"].str.contains(\"can\", regex=False) | df[\"full_text\"].str.contains(\"What\", regex=False) | df[\"full_text\"].str.contains(\"should\", regex=False)).astype(int)\n",
    "df[\"question_mark_full\"] = df[\"full_text\"].str.contains(\"?\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import add_features_to_df\n",
    "df = add_features_to_df(df.loc[df[\"is_question\"]].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import get_vectorized_inputs_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])\n",
    "\n",
    "spacy_emb_full = train_author[train_author[\"is_question\"]][\"full_text\"].apply(lambda x: nlp(x).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_features, _ = get_vectorized_inputs_and_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_features = umap_embedder.fit_transform(vectorized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_figure = get_interactive_umap_embeddings_plot(umap_features, sent_labels, raw_text, legends=sent_labels)\n",
    "# show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's cluster our data\n",
    "\n",
    "First we will try a manual approach\n",
    "\n",
    "Then, we will sweep through cluster sizes and examine silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Choose number of clusters and colormap\n",
    "n_clusters=3\n",
    "cmap = plt.get_cmap(\"Set2\")\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "\n",
    "# Fit clustering algorithm to our vectorized features\n",
    "clus = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "clusters = clus.fit_predict(vectorized_features)\n",
    "\n",
    "# Plot the dimentionality reduced features on a 2D plane\n",
    "plt.scatter(umap_features[:, 0], umap_features[:, 1], \n",
    "            c=[cmap(x/n_clusters) for x in clusters], s=40, alpha=.4)\n",
    "plt.title('UMAP projection of questions, colored by clusters', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# TODO show silhouette scores for different embedders\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "to_clus = vectorized_features\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-.3, .4])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(to_clus) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(to_clus)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(to_clus, cluster_labels, metric='cosine')\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(to_clus, cluster_labels, metric='cosine')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.3, -0.2, -0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cmap(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(umap_features[:, 0], umap_features[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centerss = clusterer.cluster_centers_\n",
    "    print(centerss.shape)\n",
    "    centers = umap_embedder.transform(centerss)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "#         a = umap_embedder.transform([c])[0]\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_editor",
   "language": "python",
   "name": "ml_editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
