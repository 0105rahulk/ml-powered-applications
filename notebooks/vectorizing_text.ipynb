{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing text\n",
    "\n",
    "As you may expect it, the goal of vectorizing text is to transform text input into a numerical representation. Ideally, this numerical representation should carry semantic meaning that make it easier for an ML model to detect patterns in a corpus of text.\n",
    "\n",
    "There are multiple ways to vectorize text. I'll demonstrate two here:\n",
    "- TF-IDF, which builds vectors for documents based on relative word frequency within the given corpus\n",
    "- Pretrained models which leverage information from other corpuses\n",
    "\n",
    "We start by loading the data, and getting a training and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel.ameisen/ml_editor/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import umap\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ml_editor.data_processing import format_raw_df, get_split_by_author\n",
    "\n",
    "data_path = Path('../data/writers.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "df = format_raw_df(df.copy())\n",
    "\n",
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on ngrams\n",
    "\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) creates embeddings based on the relative frequency of each word in each document as compared to the corpus as a whole. We create TF-IDF embeddings using sklearn below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2907\n",
       "False    2769\n",
       "Name: AcceptedAnswerId, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "questions = train_author[train_author[\"is_question\"]]\n",
    "raw_text = questions[\"body_text\"]\n",
    "# Extract a label to use as a color on our plots. \n",
    "# This label does not need to be the same label as the one for the classifier.\n",
    "sent_labels = questions[\"AcceptedAnswerId\"].notna()\n",
    "\n",
    "sent_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5676, 27381)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of a tfidf vectorizer, \n",
    "# We could use CountVectorizer for a non normalized version\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=2**21)\n",
    "\n",
    "# Fit our vectorizer to questions in our dataset\n",
    "# Returns an array of vectorized text\n",
    "bag_of_words = vectorizer.fit_transform(raw_text)\n",
    "\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now vectorized our text. This process is also called embedding, and the resulting vectors are often referred to as embeddings. We can visualize the embeddings by projecting them to two dimensions using a dimensionality reducing technique such as PCA, t-SNE or [UMAP](https://umap-learn.readthedocs.io/en/latest/). We use UMAP here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_bow = umap_embedder.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_editor.data_visualization import plot_embeddings\n",
    "\n",
    "plot_embeddings(umap_bow, sent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified representation of our data (the actual vectors are much larger than two dimensions), but can help discern trends, or with future labeling efforts. As you add features, it can be valuable to visualize embeddings to estimate whether classes look more separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings\n",
    "\n",
    "For this second approach, we load existing word vectors that were trained on a much larger corpus, which allows us to leverage general information from outside of our corpus. We will do so using the `spacy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a large model, and disable pipeline unnecessary parts for our task\n",
    "# This speeds up the vectorization process significantly\n",
    "# See https://spacy.io/models/en#en_core_web_lg for details about the model\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for each of our questions\n",
    "# By default, the vector returned is the average of all vectors in the sentence\n",
    "# See https://spacy.io/usage/vectors-similarity for more\n",
    "spacy_emb = train_author[train_author[\"is_question\"]][\"body_text\"].apply(lambda x: nlp(x).vector)\n",
    "embeddings = np.vstack(spacy_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_emb = umap_embedder.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the pretrained embeddings using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_embeddings(umap_emb, sent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topology of our dataset looks different because our embedding method is different. The underlying data however, is the same.\n",
    "\n",
    "Different embedding methods will cause a dataset to be represented differently. When using embeddings trained only on your data, documents that use the same vocabulary will be embedded close to each other. Using models trained on other corpuses however allows you to leverage information from these corpuses. With these models, semantically similar sentences may be embedded close to each other even if their vocabulary is completely different.\n",
    "\n",
    "Changing the way you vectorize your data can often have a significant impact on model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_editor",
   "language": "python",
   "name": "ml_editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
